{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fa01468cef7741e197700b28f173b0eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_60c29065282b48acb6889d34e21aad71",
              "IPY_MODEL_0e0deb35aca947caa89bfed3c25f736b",
              "IPY_MODEL_81bedeb46cdc4929aed883570ea69a74"
            ],
            "layout": "IPY_MODEL_e2ce847b5152469f911994fd6d91f86d"
          }
        },
        "60c29065282b48acb6889d34e21aad71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c618fd7139404534b605d78bf3ae75fa",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c268d27c99ec4a3d929823f1ce772dd5",
            "value": "Batches:‚Äá100%"
          }
        },
        "0e0deb35aca947caa89bfed3c25f736b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb943963e32e428fb3f8100a5e1675e2",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_436cb9800cb8452ca4411a81871975dd",
            "value": 8
          }
        },
        "81bedeb46cdc4929aed883570ea69a74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f48da226c975481da58e1f5a7197af2c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4e2bd30f61e543ce88d67b19a861a714",
            "value": "‚Äá8/8‚Äá[00:00&lt;00:00,‚Äá27.91it/s]"
          }
        },
        "e2ce847b5152469f911994fd6d91f86d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c618fd7139404534b605d78bf3ae75fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c268d27c99ec4a3d929823f1ce772dd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb943963e32e428fb3f8100a5e1675e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "436cb9800cb8452ca4411a81871975dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f48da226c975481da58e1f5a7197af2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e2bd30f61e543ce88d67b19a861a714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e70604905de6461eb40855fbaff3139c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_60894ba5f7084490baf356bca0aa262c",
              "IPY_MODEL_546d5e0d7b3244b0989a0450f6bcee94",
              "IPY_MODEL_4afaa9921a34494daa967c80f2c2592e"
            ],
            "layout": "IPY_MODEL_ebef3068305f4725b280e7db3f613a86"
          }
        },
        "60894ba5f7084490baf356bca0aa262c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c2f3fdb9232463b83ccc47dc1d66144",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ba52d4e97032499488fc31d0634b95a8",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "546d5e0d7b3244b0989a0450f6bcee94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47d6514c98af44498b35e618d8af18c7",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e843cd7646b448d8990d102280ff3fd",
            "value": 4
          }
        },
        "4afaa9921a34494daa967c80f2c2592e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_663da784ea91431693ab42bafb7c5a6f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_45103df9c53b4101b0e4de8193fcfc15",
            "value": "‚Äá4/4‚Äá[01:49&lt;00:00,‚Äá22.70s/it]"
          }
        },
        "ebef3068305f4725b280e7db3f613a86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c2f3fdb9232463b83ccc47dc1d66144": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba52d4e97032499488fc31d0634b95a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47d6514c98af44498b35e618d8af18c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e843cd7646b448d8990d102280ff3fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "663da784ea91431693ab42bafb7c5a6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45103df9c53b4101b0e4de8193fcfc15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phG-v6R_GmBc",
        "outputId": "c0a4f636-77df-49eb-edd9-e1f4ef224c2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Installing dependencies (this may take 2-3 minutes)...\n",
            "‚ö†Ô∏è  You may see some dependency warnings - these are harmless!\n",
            "\n",
            "\n",
            "‚úÖ All dependencies installed!\n",
            "üìù Note: Dependency warnings can be ignored - they don't affect functionality.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 1: Install Dependencies\n",
        "# ============================================================================\n",
        "print(\"üì¶ Installing dependencies (this may take 2-3 minutes)...\")\n",
        "print(\"‚ö†Ô∏è  You may see some dependency warnings - these are harmless!\\n\")\n",
        "\n",
        "# Install core dependencies\n",
        "!pip install -q sentence-transformers chromadb --no-warn-conflicts\n",
        "!pip install -q transformers accelerate bitsandbytes --no-warn-conflicts\n",
        "!pip install -q google-generativeai --no-warn-conflicts\n",
        "\n",
        "print(\"\\n‚úÖ All dependencies installed!\")\n",
        "print(\"üìù Note: Dependency warnings can be ignored - they don't affect functionality.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 2: Import Libraries & Check GPU\n",
        "# ============================================================================\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from google.colab import files\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "import google.generativeai as genai\n",
        "from getpass import getpass\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"‚úÖ GPU Available: {gpu_name}\")\n",
        "    print(f\"üíæ GPU Memory: {gpu_memory:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No GPU detected. Please enable GPU: Runtime > Change runtime type > T4 GPU\")\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82dKk0wbG20X",
        "outputId": "ab602ae5-c103-4b65-ceae-31b89b909e39"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ GPU Available: Tesla T4\n",
            "üíæ GPU Memory: 15.8 GB\n",
            "‚úÖ Libraries imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 3: Upload and Load Data\n",
        "# ============================================================================\n",
        "print(\"üìÅ Please upload your rupp-data.txt file...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load the JSON data\n",
        "filename = list(uploaded.keys())[0]\n",
        "with open(filename, 'r', encoding='utf-8') as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "print(f\"\\n‚úÖ Loaded {len(raw_data)} Q&A pairs\")\n",
        "print(f\"üìä Categories found: {len(set(item['category'] for item in raw_data))}\")\n",
        "print(f\"üìÇ Categories: {', '.join(sorted(set(item['category'] for item in raw_data)))}\")\n",
        "print(\"\\nüîç Sample entry:\")\n",
        "print(json.dumps(raw_data[0], indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "AR7VANxRG7Rg",
        "outputId": "9f0ba462-084c-4b45-81fd-d2ea85eacf18"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Please upload your rupp-data.txt file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fed3155a-9717-472d-ae8c-68d0f645bbee\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fed3155a-9717-472d-ae8c-68d0f645bbee\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving rupp-data.txt to rupp-data (1).txt\n",
            "\n",
            "‚úÖ Loaded 256 Q&A pairs\n",
            "üìä Categories found: 108\n",
            "üìÇ Categories: academic, academic_calendar_detail, academic_integrity, academic_probation, academic_support, adding_dropping, admission_process, admissions, after_graduation, alumni, application_process, attendance_policy, biology_program, campus, career_prep, ceremonies, changing_programs, chemistry_program, community, computer_science_detailed, contact, contact_departments, contact_specific, continuing_education, cost_living, course_load, credits, departments, development, disability, diversity, documents, double_major, electives, employment, engineering_detailed, english_detailed, english_requirements, environment, exam_format, exams, extracurricular, facilities_detail, faculty, famous_alumni, fees, fees_scholarship, financial, food, future_plans, general_info, geography_program, grade_disputes, graduation, graduation_requirements, history, history_program, homework, housing_detail, international, internships, language_support, leave_of_absence, literature_program, makeup_exams, mathematics_program, minors, online, partnerships, philosophy_program, physics_program, prerequisites, presentations, programs, programs_detail, psychology_program, publishing, quality, recognition, registration, requirements, research, research_skills, scholarships_detail, services, sociology_program, special_programs, special_requirements, specific_programs, stress_management, student_id, student_life, student_services, study_tips, summer_courses, support, teaching, teaching_methods, teaching_staff, technology, technology_resources, textbooks, time_management, transfer, transportation, vision, visiting, withdrawal\n",
            "\n",
            "üîç Sample entry:\n",
            "{\n",
            "  \"category\": \"general_info\",\n",
            "  \"question\": \"What is RUPP?\",\n",
            "  \"answer\": \"RUPP (Royal University of Phnom Penh) is Cambodia's oldest and largest national research university, established in 1960. It hosts around 30,000 students in undergraduate and postgraduate programs across sciences, humanities, social sciences, engineering, and languages. RUPP is a member of the ASEAN University Network (AUN).\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 4: Data Processing\n",
        "# ============================================================================\n",
        "def process_documents(data):\n",
        "    \"\"\"Convert raw data into structured documents\"\"\"\n",
        "    documents = []\n",
        "    for idx, item in enumerate(data):\n",
        "        doc = {\n",
        "            'id': f\"doc_{idx}\",\n",
        "            'text': f\"Question: {item['question']}\\nAnswer: {item['answer']}\",\n",
        "            'metadata': {\n",
        "                'category': item['category'],\n",
        "                'question': item['question'],\n",
        "                'answer': item['answer'],\n",
        "                'doc_id': idx\n",
        "            }\n",
        "        }\n",
        "        documents.append(doc)\n",
        "    return documents\n",
        "\n",
        "# Process documents\n",
        "documents = process_documents(raw_data)\n",
        "print(f\"‚úÖ Processed {len(documents)} documents\")\n",
        "print(f\"\\nüìÑ Sample processed document:\")\n",
        "print(f\"ID: {documents[0]['id']}\")\n",
        "print(f\"Text preview: {documents[0]['text'][:150]}...\")\n",
        "print(f\"Category: {documents[0]['metadata']['category']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G63BdghzG9uR",
        "outputId": "426c2ed5-c168-4462-c3b2-3bca91aaf0d3"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Processed 256 documents\n",
            "\n",
            "üìÑ Sample processed document:\n",
            "ID: doc_0\n",
            "Text preview: Question: What is RUPP?\n",
            "Answer: RUPP (Royal University of Phnom Penh) is Cambodia's oldest and largest national research university, established in 19...\n",
            "Category: general_info\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 5: Initialize Embedding Model (sentence-transformers)\n",
        "# ============================================================================\n",
        "print(\"ü§ñ Loading embedding model: all-MiniLM-L6-v2...\")\n",
        "print(\"‚è≥ This will take 10-20 seconds...\")\n",
        "embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "print(\"‚úÖ Embedding model loaded!\")\n",
        "\n",
        "# Test the embedder\n",
        "test_embedding = embedder.encode(\"test query\")\n",
        "print(f\"üìä Embedding dimension: {len(test_embedding)}\")\n",
        "print(f\"üìè Model size: ~80MB\")\n",
        "print(f\"‚ö° Speed: ~1000 sentences/second\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_EGl3NDHC79",
        "outputId": "8bcc23d8-4ae4-4d5a-f75c-66c3088308c7"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Loading embedding model: all-MiniLM-L6-v2...\n",
            "‚è≥ This will take 10-20 seconds...\n",
            "‚úÖ Embedding model loaded!\n",
            "üìä Embedding dimension: 384\n",
            "üìè Model size: ~80MB\n",
            "‚ö° Speed: ~1000 sentences/second\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 6: Generate Embeddings for All Documents\n",
        "# ============================================================================\n",
        "def generate_embeddings(documents, embedder):\n",
        "    \"\"\"Generate embeddings for all documents\"\"\"\n",
        "    texts = [doc['text'] for doc in documents]\n",
        "    print(f\"üîÑ Generating embeddings for {len(texts)} documents...\")\n",
        "\n",
        "    embeddings = embedder.encode(\n",
        "        texts,\n",
        "        batch_size=32,\n",
        "        show_progress_bar=True,\n",
        "        normalize_embeddings=True,\n",
        "        convert_to_numpy=True\n",
        "    )\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = generate_embeddings(documents, embedder)\n",
        "print(f\"‚úÖ Generated embeddings with shape: {embeddings.shape}\")\n",
        "print(f\"üíæ Memory usage: ~{embeddings.nbytes / 1e6:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "fa01468cef7741e197700b28f173b0eb",
            "60c29065282b48acb6889d34e21aad71",
            "0e0deb35aca947caa89bfed3c25f736b",
            "81bedeb46cdc4929aed883570ea69a74",
            "e2ce847b5152469f911994fd6d91f86d",
            "c618fd7139404534b605d78bf3ae75fa",
            "c268d27c99ec4a3d929823f1ce772dd5",
            "bb943963e32e428fb3f8100a5e1675e2",
            "436cb9800cb8452ca4411a81871975dd",
            "f48da226c975481da58e1f5a7197af2c",
            "4e2bd30f61e543ce88d67b19a861a714"
          ]
        },
        "id": "FWewjzYZHFGd",
        "outputId": "a6675d83-1950-4f4d-bf7e-020f010dead6"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Generating embeddings for 256 documents...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa01468cef7741e197700b28f173b0eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Generated embeddings with shape: (256, 384)\n",
            "üíæ Memory usage: ~0.39 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 7: Setup ChromaDB Vector Store\n",
        "# ============================================================================\n",
        "print(\"üóÑÔ∏è  Setting up ChromaDB vector database...\")\n",
        "\n",
        "# Initialize ChromaDB client\n",
        "client = chromadb.Client(Settings(\n",
        "    anonymized_telemetry=False,\n",
        "    allow_reset=True\n",
        "))\n",
        "\n",
        "# Create or reset collection\n",
        "collection_name = \"rupp_qa\"\n",
        "try:\n",
        "    client.delete_collection(collection_name)\n",
        "    print(\"üóëÔ∏è  Cleared existing collection\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "collection = client.create_collection(\n",
        "    name=collection_name,\n",
        "    metadata={\"hnsw:space\": \"cosine\"}\n",
        ")\n",
        "\n",
        "# Add documents to collection\n",
        "print(\"üíæ Adding documents to vector store...\")\n",
        "collection.add(\n",
        "    embeddings=embeddings.tolist(),\n",
        "    documents=[doc['text'] for doc in documents],\n",
        "    metadatas=[doc['metadata'] for doc in documents],\n",
        "    ids=[doc['id'] for doc in documents]\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Vector store created with {collection.count()} documents!\")\n",
        "print(f\"üîç Search algorithm: HNSW (fast approximate search)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ktBFQMAHIkr",
        "outputId": "947ba72b-2b33-433f-8221-5654b4a57e22"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üóÑÔ∏è  Setting up ChromaDB vector database...\n",
            "üóëÔ∏è  Cleared existing collection\n",
            "üíæ Adding documents to vector store...\n",
            "‚úÖ Vector store created with 256 documents!\n",
            "üîç Search algorithm: HNSW (fast approximate search)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# FLEXIBLE STAFF RETRIEVAL FUNCTION\n",
        "# ============================================================================\n",
        "def retrieve_staff_flex(query, n_results=3, category_filter=None):\n",
        "    \"\"\"\n",
        "    Retrieve top staff entries for a query.\n",
        "\n",
        "    Parameters:\n",
        "    - query: str, the question or search string\n",
        "    - n_results: int, number of top results to return\n",
        "    - category_filter: str or None, if set only searches that category\n",
        "\n",
        "    Returns:\n",
        "    - List of dicts with text, similarity, and category\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate query embedding\n",
        "    query_embedding = embedder.encode([query], normalize_embeddings=True)[0]\n",
        "\n",
        "    # Build filter if category is specified\n",
        "    where_clause = {\"category\": category_filter} if category_filter else None\n",
        "\n",
        "    # Search vector store\n",
        "    results = collection.query(\n",
        "        query_embeddings=[query_embedding.tolist()],\n",
        "        n_results=n_results,\n",
        "        where=where_clause\n",
        "    )\n",
        "\n",
        "    # Collect results\n",
        "    retrieved = []\n",
        "    for i in range(len(results['documents'][0])):\n",
        "        retrieved.append({\n",
        "            \"text\": results['documents'][0][i],\n",
        "            \"similarity\": 1 - results['distances'][0][i] if results['distances'] else None,\n",
        "            \"category\": results['metadatas'][0][i].get(\"category\", \"unknown\"),\n",
        "            \"source_file\": results['metadatas'][0][i].get(\"source_file\", \"unknown\"),\n",
        "            \"chunk_index\": results['metadatas'][0][i].get(\"chunk_index\", 0),\n",
        "            \"total_chunks\": results['metadatas'][0][i].get(\"total_chunks\", 0)\n",
        "        })\n",
        "\n",
        "    return retrieved\n"
      ],
      "metadata": {
        "id": "kBVE4OfkHMcJ"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate with HuggingFace\n",
        "from huggingface_hub import login\n",
        "from getpass import getpass\n",
        "\n",
        "token = getpass(\"Enter your HuggingFace token: \")\n",
        "login(token=token)\n",
        "print(\"‚úÖ Authenticated!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m1FelTpJUhT",
        "outputId": "cf3ac6c6-73f0-490a-fb57-72cb8dcc369e"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your HuggingFace token: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "‚úÖ Authenticated!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 9: Load Llama-3.1-8B-Instruct (Primary LLM)\n",
        "# ============================================================================\n",
        "print(\"ü¶ô Loading Llama-3.1-8B-Instruct...\")\n",
        "print(\"‚è≥ This will take 2-3 minutes (one-time download)...\")\n",
        "print(\"üíæ Model will use ~6GB GPU memory with 4-bit quantization\")\n",
        "\n",
        "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "\n",
        "# Configure 4-bit quantization to fit in free Colab GPU\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model with quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Llama-3.1-8B loaded successfully!\")\n",
        "print(f\"üìä Model size: ~4.5GB (4-bit quantized)\")\n",
        "print(f\"‚ö° Expected speed: 40-60 tokens/second\")\n",
        "print(f\"üéØ Quality: Near GPT-3.5 level (88% benchmark)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170,
          "referenced_widgets": [
            "e70604905de6461eb40855fbaff3139c",
            "60894ba5f7084490baf356bca0aa262c",
            "546d5e0d7b3244b0989a0450f6bcee94",
            "4afaa9921a34494daa967c80f2c2592e",
            "ebef3068305f4725b280e7db3f613a86",
            "0c2f3fdb9232463b83ccc47dc1d66144",
            "ba52d4e97032499488fc31d0634b95a8",
            "47d6514c98af44498b35e618d8af18c7",
            "7e843cd7646b448d8990d102280ff3fd",
            "663da784ea91431693ab42bafb7c5a6f",
            "45103df9c53b4101b0e4de8193fcfc15"
          ]
        },
        "id": "otlwjRckHPlV",
        "outputId": "ba1f91e2-1550-438d-ea59-4e4bca082553"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶ô Loading Llama-3.1-8B-Instruct...\n",
            "‚è≥ This will take 2-3 minutes (one-time download)...\n",
            "üíæ Model will use ~6GB GPU memory with 4-bit quantization\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e70604905de6461eb40855fbaff3139c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Llama-3.1-8B loaded successfully!\n",
            "üìä Model size: ~4.5GB (4-bit quantized)\n",
            "‚ö° Expected speed: 40-60 tokens/second\n",
            "üéØ Quality: Near GPT-3.5 level (88% benchmark)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 10: Generate Answer Function with Llama-3.1\n",
        "# ============================================================================\n",
        "def generate_answer_llama(query, context_docs, max_tokens=256, temperature=0.3):\n",
        "    \"\"\"Generate answer using Llama-3.1-8B\"\"\"\n",
        "\n",
        "    # Build context from retrieved documents\n",
        "    context = \"\\n\\n\".join([\n",
        "        f\"Reference {i+1}:\\n{doc['text']}\"\n",
        "        for i, doc in enumerate(context_docs)\n",
        "    ])\n",
        "\n",
        "    # Create chat messages in Llama-3.1 format\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful academic advisor at RUPP (Royal University of Phnom Penh). Provide clear, accurate, and professional information based on the context given. Keep answers concise but complete.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"Context information from RUPP policies:\n",
        "{context}\n",
        "\n",
        "Student Question: {query}\n",
        "\n",
        "Instructions:\n",
        "- Provide a clear and accurate answer based ONLY on the context above\n",
        "- If the context doesn't contain enough information, say so\n",
        "- Keep the tone professional and helpful\n",
        "- Be specific and cite relevant policies when applicable\"\"\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Format prompt using Llama-3.1 chat template\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode response\n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the assistant's response (after the last \"assistant\" marker)\n",
        "    if \"assistant\" in full_response:\n",
        "        response = full_response.split(\"assistant\")[-1].strip()\n",
        "    else:\n",
        "        response = full_response.strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "print(\"‚úÖ Llama-3.1 generation function ready!\")\n",
        "\n",
        "# Test generation\n",
        "print(\"\\nüß™ Testing Llama-3.1 generation...\")\n",
        "test_context = retrieve_context(\"Can I take fewer courses?\", n_results=2)\n",
        "test_answer = generate_answer_llama(\"Can I study part-time?\", test_context, max_tokens=150)\n",
        "print(f\"\\nüí¨ Test Answer:\\n{test_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TaFbn52HSqy",
        "outputId": "870fc5eb-19cf-479d-929c-3e77c876c381"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Llama-3.1 generation function ready!\n",
            "\n",
            "üß™ Testing Llama-3.1 generation...\n",
            "\n",
            "üí¨ Test Answer:\n",
            "According to RUPP's policies, part-time enrollment with fewer courses may be possible. However, this would extend your graduation time. If you are interested in taking fewer courses, I recommend checking with the Studies Office to discuss your part-time status and any implications for scholarships.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 12: Complete RAG Pipeline\n",
        "# ============================================================================\n",
        "def rag_query(user_query, category=None, n_results=3, use_gemini=False, max_tokens=256):\n",
        "    \"\"\"Complete RAG pipeline with Llama-3.1 or Gemini\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üîç Query: {user_query}\")\n",
        "    if category:\n",
        "        print(f\"üìÇ Category filter: {category}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Step 1: Retrieve relevant context\n",
        "    print(\"üìö Step 1: Retrieving relevant documents...\")\n",
        "    context_docs = retrieve_context(\n",
        "        query=user_query,\n",
        "        n_results=n_results,\n",
        "        category_filter=category\n",
        "    )\n",
        "\n",
        "    print(f\"‚úÖ Retrieved {len(context_docs)} documents\")\n",
        "    for i, doc in enumerate(context_docs, 1):\n",
        "        sim_score = 1 - doc['distance']\n",
        "        print(f\"   {i}. [{doc['metadata']['category']}] Similarity: {sim_score:.3f}\")\n",
        "\n",
        "    # Step 2: Generate answer\n",
        "    print(f\"\\nü§ñ Step 2: Generating answer with {'Gemini' if use_gemini else 'Llama-3.1'}...\")\n",
        "\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "\n",
        "    if use_gemini and generate_answer_gemini:\n",
        "        answer = generate_answer_gemini(user_query, context_docs)\n",
        "    else:\n",
        "        answer = generate_answer_llama(user_query, context_docs, max_tokens=max_tokens)\n",
        "\n",
        "    generation_time = time.time() - start_time\n",
        "\n",
        "    print(f\"‚úÖ Answer generated in {generation_time:.2f} seconds\")\n",
        "\n",
        "    # Step 3: Format response\n",
        "    result = {\n",
        "        'query': user_query,\n",
        "        'answer': answer,\n",
        "        'sources': [\n",
        "            {\n",
        "                'category': doc['metadata']['category'],\n",
        "                'question': doc['metadata']['question'],\n",
        "                'similarity': 1 - doc['distance']\n",
        "            }\n",
        "            for doc in context_docs\n",
        "        ],\n",
        "        'num_sources': len(context_docs),\n",
        "        'generation_time': generation_time,\n",
        "        'model_used': 'Gemini' if use_gemini else 'Llama-3.1-8B'\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "print(\"‚úÖ Complete RAG pipeline ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhDJlzxpHb4U",
        "outputId": "d57797d6-5607-4d6a-9da4-e585b5b6677f"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Complete RAG pipeline ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 13: Test RAG System with Multiple Queries\n",
        "# ============================================================================\n",
        "# Test queries covering different categories\n",
        "test_queries = [\n",
        "    \"Can I take fewer courses?\",\n",
        "    \"Are there summer classes?\",\n",
        "    \"What happens if I fail a course?\",\n",
        "    \"How do I withdraw from a class?\",\n",
        "]\n",
        "\n",
        "print(\"üß™ TESTING RAG SYSTEM\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for i, query in enumerate(test_queries, 1):\n",
        "    print(f\"\\n{'#'*70}\")\n",
        "    print(f\"TEST {i}/{len(test_queries)}\")\n",
        "    print(f\"{'#'*70}\")\n",
        "\n",
        "    result = rag_query(query, use_gemini=False, n_results=3, max_tokens=200)\n",
        "\n",
        "    print(f\"\\nüí¨ ANSWER:\")\n",
        "    print(result['answer'])\n",
        "\n",
        "    print(f\"\\nüìñ SOURCES ({result['num_sources']}):\")\n",
        "    for j, source in enumerate(result['sources'], 1):\n",
        "        print(f\"   {j}. [{source['category']}] {source['question'][:60]}...\")\n",
        "        print(f\"      Similarity: {source['similarity']:.3f}\")\n",
        "\n",
        "    print(f\"\\n‚è±Ô∏è  Generation time: {result['generation_time']:.2f}s\")\n",
        "    print(f\"ü§ñ Model: {result['model_used']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ All tests completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcvDv0rFHe4X",
        "outputId": "f8e04fac-36d4-4878-cc3d-1dff3e1ff5c8"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ TESTING RAG SYSTEM\n",
            "======================================================================\n",
            "\n",
            "######################################################################\n",
            "TEST 1/4\n",
            "######################################################################\n",
            "\n",
            "======================================================================\n",
            "üîç Query: Can I take fewer courses?\n",
            "======================================================================\n",
            "\n",
            "üìö Step 1: Retrieving relevant documents...\n",
            "‚úÖ Retrieved 3 documents\n",
            "   1. [course_load] Similarity: 0.761\n",
            "   2. [course_load] Similarity: 0.628\n",
            "   3. [attendance_policy] Similarity: 0.579\n",
            "\n",
            "ü§ñ Step 2: Generating answer with Llama-3.1...\n",
            "‚úÖ Answer generated in 16.24 seconds\n",
            "\n",
            "üí¨ ANSWER:\n",
            "Based on the provided context, it appears that part-time enrollment with fewer courses is possible, but it may extend your graduation time. To confirm this, I recommend checking with the Studies Office about part-time status and any implications for scholarships.\n",
            "\n",
            "Please note that taking too many courses can hurt your grades and understanding, and excessive absences can result in being barred from taking final exams or failing the course. However, the context does not provide specific information on the minimum number of courses required to maintain full-time status.\n",
            "\n",
            "To ensure you have the most accurate and up-to-date information, I suggest consulting with the Studies Office or your program's coordinator for guidance on the recommended course load and any specific requirements for your program.\n",
            "\n",
            "üìñ SOURCES (3):\n",
            "   1. [course_load] Can I take fewer courses?...\n",
            "      Similarity: 0.761\n",
            "   2. [course_load] How many courses should I take per semester?...\n",
            "      Similarity: 0.628\n",
            "   3. [attendance_policy] What happens if I miss too many classes?...\n",
            "      Similarity: 0.579\n",
            "\n",
            "‚è±Ô∏è  Generation time: 16.24s\n",
            "ü§ñ Model: Llama-3.1-8B\n",
            "\n",
            "######################################################################\n",
            "TEST 2/4\n",
            "######################################################################\n",
            "\n",
            "======================================================================\n",
            "üîç Query: Are there summer classes?\n",
            "======================================================================\n",
            "\n",
            "üìö Step 1: Retrieving relevant documents...\n",
            "‚úÖ Retrieved 3 documents\n",
            "   1. [summer_courses] Similarity: 0.789\n",
            "   2. [extracurricular] Similarity: 0.572\n",
            "   3. [teaching_methods] Similarity: 0.418\n",
            "\n",
            "ü§ñ Step 2: Generating answer with Llama-3.1...\n",
            "‚úÖ Answer generated in 13.45 seconds\n",
            "\n",
            "üí¨ ANSWER:\n",
            "According to RUPP policies, some programs offer summer courses for catching up, getting ahead, or retaking failed courses. These summer sessions are more intensive with compressed schedules. I recommend checking with your department about summer offerings for more information.\n",
            "\n",
            "üìñ SOURCES (3):\n",
            "   1. [summer_courses] Are there summer classes?...\n",
            "      Similarity: 0.789\n",
            "   2. [extracurricular] What activities are available outside class?...\n",
            "      Similarity: 0.572\n",
            "   3. [teaching_methods] Is there group work in classes?...\n",
            "      Similarity: 0.418\n",
            "\n",
            "‚è±Ô∏è  Generation time: 13.45s\n",
            "ü§ñ Model: Llama-3.1-8B\n",
            "\n",
            "######################################################################\n",
            "TEST 3/4\n",
            "######################################################################\n",
            "\n",
            "======================================================================\n",
            "üîç Query: What happens if I fail a course?\n",
            "======================================================================\n",
            "\n",
            "üìö Step 1: Retrieving relevant documents...\n",
            "‚úÖ Retrieved 3 documents\n",
            "   1. [exams] Similarity: 0.752\n",
            "   2. [attendance_policy] Similarity: 0.626\n",
            "   3. [makeup_exams] Similarity: 0.599\n",
            "\n",
            "ü§ñ Step 2: Generating answer with Llama-3.1...\n",
            "‚úÖ Answer generated in 11.41 seconds\n",
            "\n",
            "üí¨ ANSWER:\n",
            "Based on the provided context, if you fail a course at RUPP, the outcome depends on the specific policies of your program. According to Reference 1, you may need to retake the course or sit for a supplementary exam, depending on your program's policies. It is best to contact your department office for specific procedures.\n",
            "\n",
            "Additionally, as mentioned in Reference 3, retake policies vary by course and program, so it's essential to check with your department about specific policies regarding failed courses.\n",
            "\n",
            "üìñ SOURCES (3):\n",
            "   1. [exams] What happens if I fail an exam?...\n",
            "      Similarity: 0.752\n",
            "   2. [attendance_policy] What happens if I miss too many classes?...\n",
            "      Similarity: 0.626\n",
            "   3. [makeup_exams] Can I retake a failed exam?...\n",
            "      Similarity: 0.599\n",
            "\n",
            "‚è±Ô∏è  Generation time: 11.41s\n",
            "ü§ñ Model: Llama-3.1-8B\n",
            "\n",
            "######################################################################\n",
            "TEST 4/4\n",
            "######################################################################\n",
            "\n",
            "======================================================================\n",
            "üîç Query: How do I withdraw from a class?\n",
            "======================================================================\n",
            "\n",
            "üìö Step 1: Retrieving relevant documents...\n",
            "‚úÖ Retrieved 3 documents\n",
            "   1. [withdrawal] Similarity: 0.556\n",
            "   2. [academic_support] Similarity: 0.535\n",
            "   3. [withdrawal] Similarity: 0.530\n",
            "\n",
            "ü§ñ Step 2: Generating answer with Llama-3.1...\n",
            "‚úÖ Answer generated in 20.71 seconds\n",
            "\n",
            "üí¨ ANSWER:\n",
            "Based on the provided context, I'm happy to assist you with your question. \n",
            "\n",
            "According to the RUPP policies, if you need to withdraw from a class, I recommend that you follow the official withdrawal procedures through the Studies Office. This is because the policies mention that withdrawing from university affects your academic record, financial obligations, and re-enrollment eligibility.\n",
            "\n",
            "However, the provided context does not explicitly state the step-by-step process for withdrawing from a class. Therefore, I suggest contacting the Studies Office directly for the most up-to-date and detailed information on the withdrawal process.\n",
            "\n",
            "Before proceeding with withdrawal, it's also a good idea to discuss your situation with your academic advisor to understand the potential implications on your academic record and future re-enrollment eligibility.\n",
            "\n",
            "Please note that the RUPP policies do not provide a clear procedure for withdrawing from a single class, but rather from the university as a whole. If you need to withdraw from a specific class, it's best to consult with the Studies Office for\n",
            "\n",
            "üìñ SOURCES (3):\n",
            "   1. [withdrawal] What if I need to withdraw from university?...\n",
            "      Similarity: 0.556\n",
            "   2. [academic_support] What if I'm struggling with classes?...\n",
            "      Similarity: 0.535\n",
            "   3. [withdrawal] Can I come back after withdrawing?...\n",
            "      Similarity: 0.530\n",
            "\n",
            "‚è±Ô∏è  Generation time: 20.71s\n",
            "ü§ñ Model: Llama-3.1-8B\n",
            "\n",
            "======================================================================\n",
            "‚úÖ All tests completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 14: Interactive Chat Interface\n",
        "# ============================================================================\n",
        "def interactive_rag():\n",
        "    \"\"\"Interactive query interface for RUPP chatbot\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üéì RUPP Q&A CHATBOT - Interactive Mode\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"Commands:\")\n",
        "    print(\"  ‚Ä¢ Type your question to get an answer\")\n",
        "    print(\"  ‚Ä¢ 'categories' - Show all available categories\")\n",
        "    print(\"  ‚Ä¢ 'stats' - Show system statistics\")\n",
        "    print(\"  ‚Ä¢ 'switch' - Switch between Llama and Gemini\")\n",
        "    print(\"  ‚Ä¢ 'quit' - Exit the chatbot\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    # Get available categories\n",
        "    categories = sorted(set(doc['metadata']['category'] for doc in documents))\n",
        "    use_gemini = False\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"\\n‚ùì Your question: \").strip()\n",
        "\n",
        "            if not user_input:\n",
        "                continue\n",
        "\n",
        "            if user_input.lower() == 'quit':\n",
        "                print(\"üëã Thank you for using RUPP Q&A Chatbot. Goodbye!\")\n",
        "                break\n",
        "\n",
        "            if user_input.lower() == 'categories':\n",
        "                print(f\"\\nüìÇ Available categories ({len(categories)}):\")\n",
        "                for i, cat in enumerate(categories, 1):\n",
        "                    count = sum(1 for d in documents if d['metadata']['category'] == cat)\n",
        "                    print(f\"   {i}. {cat} ({count} Q&As)\")\n",
        "                continue\n",
        "\n",
        "            if user_input.lower() == 'stats':\n",
        "                print(f\"\\nüìä System Statistics:\")\n",
        "                print(f\"   ‚Ä¢ Total Q&A pairs: {len(documents)}\")\n",
        "                print(f\"   ‚Ä¢ Categories: {len(categories)}\")\n",
        "                print(f\"   ‚Ä¢ Embedding model: all-MiniLM-L6-v2 (384d)\")\n",
        "                print(f\"   ‚Ä¢ LLM model: {'Gemini Pro' if use_gemini else 'Llama-3.1-8B'}\")\n",
        "                print(f\"   ‚Ä¢ Vector DB: ChromaDB (HNSW)\")\n",
        "                continue\n",
        "\n",
        "            if user_input.lower() == 'switch':\n",
        "                if generate_answer_gemini:\n",
        "                    use_gemini = not use_gemini\n",
        "                    print(f\"üîÑ Switched to {'Gemini Pro' if use_gemini else 'Llama-3.1-8B'}\")\n",
        "                else:\n",
        "                    print(\"‚ö†Ô∏è  Gemini not configured. Using Llama-3.1 only.\")\n",
        "                continue\n",
        "\n",
        "            # Ask for optional category filter\n",
        "            filter_cat = input(\"üìÇ Filter by category? (press Enter to skip): \").strip()\n",
        "            category_filter = filter_cat if filter_cat and filter_cat in categories else None\n",
        "\n",
        "            if filter_cat and filter_cat not in categories and filter_cat != \"\":\n",
        "                print(f\"‚ö†Ô∏è  Category '{filter_cat}' not found. Searching all categories...\")\n",
        "                category_filter = None\n",
        "\n",
        "            # Run RAG query\n",
        "            result = rag_query(\n",
        "                user_input,\n",
        "                category=category_filter,\n",
        "                use_gemini=use_gemini,\n",
        "                n_results=3,\n",
        "                max_tokens=250\n",
        "            )\n",
        "\n",
        "            print(f\"\\nüí¨ ANSWER:\")\n",
        "            print(result['answer'])\n",
        "\n",
        "            print(f\"\\nüìñ SOURCES:\")\n",
        "            for i, source in enumerate(result['sources'], 1):\n",
        "                print(f\"   {i}. [{source['category']}] {source['question']}\")\n",
        "                print(f\"      Relevance: {source['similarity']:.1%}\")\n",
        "\n",
        "            print(f\"\\n‚è±Ô∏è  Response time: {result['generation_time']:.2f}s\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\nüëã Interrupted. Goodbye!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå Error: {str(e)}\")\n",
        "            print(\"Please try again or type 'quit' to exit.\")\n",
        "\n",
        "# Run interactive mode\n",
        "print(\"\\nüöÄ Starting interactive chatbot...\")\n",
        "interactive_rag()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_MN93d1HiNb",
        "outputId": "99a4c482-70ef-4558-9edf-702d22b2c3ad"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ Starting interactive chatbot...\n",
            "\n",
            "======================================================================\n",
            "üéì RUPP Q&A CHATBOT - Interactive Mode\n",
            "======================================================================\n",
            "Commands:\n",
            "  ‚Ä¢ Type your question to get an answer\n",
            "  ‚Ä¢ 'categories' - Show all available categories\n",
            "  ‚Ä¢ 'stats' - Show system statistics\n",
            "  ‚Ä¢ 'switch' - Switch between Llama and Gemini\n",
            "  ‚Ä¢ 'quit' - Exit the chatbot\n",
            "======================================================================\n",
            "\n",
            "\n",
            "‚ùì Your question: quit\n",
            "üëã Thank you for using RUPP Q&A Chatbot. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 15: Evaluation & Performance Metrics\n",
        "# ============================================================================\n",
        "def evaluate_retrieval(test_cases):\n",
        "    \"\"\"Evaluate retrieval quality\"\"\"\n",
        "\n",
        "    print(\"\\nüìä RETRIEVAL EVALUATION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    total_correct = 0\n",
        "    results = []\n",
        "\n",
        "    for test in test_cases:\n",
        "        query = test['query']\n",
        "        expected_category = test['expected_category']\n",
        "\n",
        "        # Retrieve top result\n",
        "        context_docs = retrieve_context(query, n_results=1)\n",
        "        retrieved_category = context_docs[0]['metadata']['category']\n",
        "        similarity = 1 - context_docs[0]['distance']\n",
        "\n",
        "        is_correct = retrieved_category == expected_category\n",
        "        total_correct += is_correct\n",
        "\n",
        "        results.append({\n",
        "            'query': query,\n",
        "            'expected': expected_category,\n",
        "            'retrieved': retrieved_category,\n",
        "            'correct': is_correct,\n",
        "            'similarity': similarity\n",
        "        })\n",
        "\n",
        "        status = \"‚úÖ\" if is_correct else \"‚ùå\"\n",
        "        print(f\"{status} {query[:45]:45} | Expected: {expected_category:15} | Got: {retrieved_category:15} | Sim: {similarity:.3f}\")\n",
        "\n",
        "    accuracy = total_correct / len(test_cases) * 100\n",
        "    avg_similarity = np.mean([r['similarity'] for r in results])\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üéØ Retrieval Accuracy: {accuracy:.1f}% ({total_correct}/{len(test_cases)})\")\n",
        "    print(f\"üìä Average Similarity: {avg_similarity:.3f}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example test cases (customize with your actual data)\n",
        "example_test_cases = [\n",
        "    {'query': 'Can I take fewer courses?', 'expected_category': 'course_load'},\n",
        "    {'query': 'Are there summer classes?', 'expected_category': 'summer_courses'},\n",
        "]\n",
        "\n",
        "print(\"\\nüí° To run evaluation, create test cases and call:\")\n",
        "print(\"eval_results = evaluate_retrieval(your_test_cases)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZQd9mxiHljn",
        "outputId": "d7f19f30-5946-413f-8dbf-72983418f374"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üí° To run evaluation, create test cases and call:\n",
            "eval_results = evaluate_retrieval(your_test_cases)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 16: Save & Export Results\n",
        "# ============================================================================\n",
        "def save_conversation(queries_and_answers, filename='rag_conversation.json'):\n",
        "    \"\"\"Save Q&A results to file and download\"\"\"\n",
        "\n",
        "    # Prepare data for export\n",
        "    export_data = {\n",
        "        'system_info': {\n",
        "            'embedding_model': 'sentence-transformers/all-MiniLM-L6-v2',\n",
        "            'llm_model': 'meta-llama/Meta-Llama-3.1-8B-Instruct',\n",
        "            'vector_db': 'ChromaDB',\n",
        "            'total_documents': len(documents),\n",
        "            'retrieval_k': 3\n",
        "        },\n",
        "        'conversations': queries_and_answers\n",
        "    }\n",
        "\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"‚úÖ Results saved to {filename}\")\n",
        "\n",
        "    # Download file\n",
        "    files.download(filename)\n",
        "    print(f\"üì• File downloaded!\")\n",
        "\n",
        "def export_to_csv(results, filename='rag_results.csv'):\n",
        "    \"\"\"Export results to CSV format\"\"\"\n",
        "    import csv\n",
        "\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['Query', 'Answer', 'Model', 'Time', 'Top_Category', 'Similarity'])\n",
        "\n",
        "        for r in results:\n",
        "            writer.writerow([\n",
        "                r['query'],\n",
        "                r['answer'],\n",
        "                r['model_used'],\n",
        "                f\"{r['generation_time']:.2f}\",\n",
        "                r['sources'][0]['category'] if r['sources'] else 'N/A',\n",
        "                f\"{r['sources'][0]['similarity']:.3f}\" if r['sources'] else 'N/A'\n",
        "            ])\n",
        "\n",
        "    print(f\"‚úÖ Exported to {filename}\")\n",
        "    files.download(filename)\n",
        "\n",
        "print(\"\\nüí° To save your results:\")\n",
        "print(\"save_conversation(your_results)\")\n",
        "print(\"export_to_csv(your_results)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-obdtiwHsJz",
        "outputId": "3a28c70c-97b6-4e5b-dd2c-bbec84d3492d"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üí° To save your results:\n",
            "save_conversation(your_results)\n",
            "export_to_csv(your_results)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 17: Install FastAPI & Expose to Public\n",
        "# ============================================================================\n",
        "!pip install -q fastapi uvicorn pyngrok python-multipart\n",
        "print(\"‚úÖ FastAPI dependencies installed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sohwhlo0U-SR",
        "outputId": "b40eb798-5f7f-4354-947d-2ed48e994a16"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ FastAPI dependencies installed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 18: FastAPI Server for Next.js\n",
        "# ============================================================================\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "from typing import List, Optional\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "\n",
        "# Allow nested event loops (required for Colab)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Initialize FastAPI\n",
        "app = FastAPI(title=\"RUPP RAG API\", version=\"1.0.0\")\n",
        "\n",
        "# Enable CORS for Next.js\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # In production, specify your domain\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Request/Response Models\n",
        "class QueryRequest(BaseModel):\n",
        "    question: str\n",
        "    category: Optional[str] = None\n",
        "    n_results: Optional[int] = 3\n",
        "    use_gemini: Optional[bool] = False\n",
        "\n",
        "class Source(BaseModel):\n",
        "    category: str\n",
        "    question: str\n",
        "    similarity: float\n",
        "\n",
        "class QueryResponse(BaseModel):\n",
        "    query: str\n",
        "    answer: str\n",
        "    sources: List[Source]\n",
        "    generation_time: float\n",
        "    model_used: str\n",
        "\n",
        "# Health check endpoint\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "    return {\n",
        "        \"status\": \"online\",\n",
        "        \"message\": \"RUPP RAG API\",\n",
        "        \"endpoints\": {\n",
        "            \"/query\": \"POST - Ask a question\",\n",
        "            \"/categories\": \"GET - List all categories\",\n",
        "            \"/health\": \"GET - Check system health\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Query endpoint\n",
        "@app.post(\"/query\", response_model=QueryResponse)\n",
        "async def query_endpoint(request: QueryRequest):\n",
        "    try:\n",
        "        # Run RAG query\n",
        "        result = rag_query(\n",
        "            user_query=request.question,\n",
        "            category=request.category,\n",
        "            n_results=request.n_results,\n",
        "            use_gemini=request.use_gemini,\n",
        "            max_tokens=256\n",
        "        )\n",
        "\n",
        "        return QueryResponse(**result)\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "# Get categories endpoint\n",
        "@app.get(\"/categories\")\n",
        "def get_categories():\n",
        "    categories = sorted(set(doc['metadata']['category'] for doc in documents))\n",
        "    return {\n",
        "        \"categories\": categories,\n",
        "        \"total\": len(categories)\n",
        "    }\n",
        "\n",
        "# Health check\n",
        "@app.get(\"/health\")\n",
        "def health_check():\n",
        "    return {\n",
        "        \"status\": \"healthy\",\n",
        "        \"model\": \"Llama-3.1-8B\",\n",
        "        \"documents\": len(documents),\n",
        "        \"gpu_available\": torch.cuda.is_available()\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ FastAPI app configured!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeDaaoNAVIcV",
        "outputId": "2ee3f482-c6ac-4ca1-f62c-8ca56ae669c4"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ FastAPI app configured!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 19: Start Server & Get Public URL  (COLAB SAFE VERSION)\n",
        "# ============================================================================\n",
        "from getpass import getpass\n",
        "import threading\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Patch the running event loop so uvicorn works in Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Get ngrok auth token (free: https://dashboard.ngrok.com/get-started/your-authtoken)\n",
        "print(\"üîë Get your FREE ngrok token from: https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "ngrok_token = getpass(\"Enter ngrok auth token: \")\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "\n",
        "# Kill previous ngrok if exists\n",
        "!pkill ngrok || echo \"No existing ngrok process.\"\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"üåê PUBLIC API URL: {public_url}\")\n",
        "print(f\"{'='*70}\")\n",
        "print(\"\\nüìù Copy this URL for your Next.js app!\\n\")\n",
        "\n",
        "print(\"üß™ Test endpoints:\")\n",
        "print(f\"   ‚Ä¢ Health: {public_url}/health\")\n",
        "print(f\"   ‚Ä¢ Categories: {public_url}/categories\")\n",
        "print(f\"   ‚Ä¢ Query: {public_url}/query  (POST)\")\n",
        "print(\"\\n‚ö†Ô∏è  Keep this cell running! Server will stop if you interrupt it.\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Run Uvicorn in a background thread\n",
        "# ------------------------------\n",
        "def start_server():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "server_thread = threading.Thread(target=start_server, daemon=True)\n",
        "server_thread.start()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rc1My3WrVQHy",
        "outputId": "068f3f45-38cf-4a04-990e-54f8dc1f4726"
      },
      "execution_count": 74,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîë Get your FREE ngrok token from: https://dashboard.ngrok.com/get-started/your-authtoken\n",
            "Enter ngrok auth token: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-11-16T09:41:03+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8000-b891b817-8fdd-45e6-90e7-87403896441a acceptErr=\"failed to accept connection: Listener closed\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "üåê PUBLIC API URL: NgrokTunnel: \"https://grouseless-nonphysically-craig.ngrok-free.dev\" -> \"http://localhost:8000\"\n",
            "======================================================================\n",
            "\n",
            "üìù Copy this URL for your Next.js app!\n",
            "\n",
            "üß™ Test endpoints:\n",
            "   ‚Ä¢ Health: NgrokTunnel: \"https://grouseless-nonphysically-craig.ngrok-free.dev\" -> \"http://localhost:8000\"/health\n",
            "   ‚Ä¢ Categories: NgrokTunnel: \"https://grouseless-nonphysically-craig.ngrok-free.dev\" -> \"http://localhost:8000\"/categories\n",
            "   ‚Ä¢ Query: NgrokTunnel: \"https://grouseless-nonphysically-craig.ngrok-free.dev\" -> \"http://localhost:8000\"/query  (POST)\n",
            "\n",
            "‚ö†Ô∏è  Keep this cell running! Server will stop if you interrupt it.\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [1341]\n",
            "INFO:     Waiting for application startup.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SINGLE CELL: Complete Document Upload API Setup\n",
        "# ============================================================================\n",
        "\n",
        "# Install dependencies\n",
        "print(\"üì¶ Installing document processing libraries...\")\n",
        "!pip install -q PyPDF2 python-docx\n",
        "print(\"‚úÖ Libraries installed!\")\n",
        "\n",
        "# Import required modules\n",
        "import PyPDF2\n",
        "import docx\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Optional\n",
        "from fastapi import File, UploadFile, Form\n",
        "from fastapi.responses import JSONResponse\n",
        "import os\n",
        "\n",
        "# Document Processor Class\n",
        "class DocumentProcessor:\n",
        "    @staticmethod\n",
        "    def extract_from_pdf(file_path: str) -> str:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            pdf_reader = PyPDF2.PdfReader(f)\n",
        "            return \"\\n\\n\".join([page.extract_text() for page in pdf_reader.pages]).strip()\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_from_docx(file_path: str) -> str:\n",
        "        doc = docx.Document(file_path)\n",
        "        return \"\\n\\n\".join([para.text for para in doc.paragraphs if para.text.strip()])\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_from_txt(file_path: str) -> str:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            return f.read()\n",
        "\n",
        "    @staticmethod\n",
        "    def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
        "        words = text.split()\n",
        "        chunks = []\n",
        "        for i in range(0, len(words), chunk_size - overlap):\n",
        "            chunk = ' '.join(words[i:i + chunk_size])\n",
        "            if chunk.strip():\n",
        "                chunks.append(chunk)\n",
        "        return chunks\n",
        "\n",
        "    @staticmethod\n",
        "    def process_document(file_path: str, category: str = \"general\") -> List[Dict]:\n",
        "        # Extract text based on file type\n",
        "        if file_path.endswith('.pdf'):\n",
        "            text = DocumentProcessor.extract_from_pdf(file_path)\n",
        "        elif file_path.endswith('.docx'):\n",
        "            text = DocumentProcessor.extract_from_docx(file_path)\n",
        "        elif file_path.endswith('.txt'):\n",
        "            text = DocumentProcessor.extract_from_txt(file_path)\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "        if not text:\n",
        "            return []\n",
        "\n",
        "        # Chunk text\n",
        "        chunks = DocumentProcessor.chunk_text(text)\n",
        "\n",
        "        # Create documents\n",
        "        documents = []\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            documents.append({\n",
        "                'text': chunk,\n",
        "                'metadata': {\n",
        "                    'category': category,\n",
        "                    'source_file': os.path.basename(file_path),\n",
        "                    'chunk_index': i,\n",
        "                    'total_chunks': len(chunks),\n",
        "                    'upload_date': datetime.now().isoformat()\n",
        "                }\n",
        "            })\n",
        "\n",
        "        return documents\n",
        "\n",
        "# Vector Store Manager\n",
        "class VectorStoreManager:\n",
        "    def __init__(self, collection, embedder):\n",
        "        self.collection = collection\n",
        "        self.embedder = embedder\n",
        "\n",
        "    def add_documents(self, documents: List[Dict], category: str = \"general\"):\n",
        "        existing_count = self.collection.count()\n",
        "        new_docs = []\n",
        "\n",
        "        for i, doc in enumerate(documents):\n",
        "            doc['id'] = f\"doc_{existing_count + i}\"\n",
        "            if 'category' not in doc['metadata']:\n",
        "                doc['metadata']['category'] = category\n",
        "            new_docs.append(doc)\n",
        "\n",
        "        texts = [doc['text'] for doc in new_docs]\n",
        "        embeddings = self.embedder.encode(texts, normalize_embeddings=True, show_progress_bar=True)\n",
        "\n",
        "        self.collection.add(\n",
        "            embeddings=embeddings.tolist(),\n",
        "            documents=texts,\n",
        "            metadatas=[doc['metadata'] for doc in new_docs],\n",
        "            ids=[doc['id'] for doc in new_docs]\n",
        "        )\n",
        "\n",
        "        return len(new_docs)\n",
        "\n",
        "    def get_stats(self):\n",
        "        total_docs = self.collection.count()\n",
        "        all_docs = self.collection.get()\n",
        "\n",
        "        categories = {}\n",
        "        sources = {}\n",
        "\n",
        "        if all_docs and all_docs['metadatas']:\n",
        "            for meta in all_docs['metadatas']:\n",
        "                cat = meta.get('category', 'unknown')\n",
        "                src = meta.get('source_file', 'unknown')\n",
        "                categories[cat] = categories.get(cat, 0) + 1\n",
        "                sources[src] = sources.get(src, 0) + 1\n",
        "\n",
        "        return {\n",
        "            'total_documents': total_docs,\n",
        "            'categories': categories,\n",
        "            'sources': sources,\n",
        "            'last_update': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "    def delete_by_category(self, category: str):\n",
        "        self.collection.delete(where={'category': category})\n",
        "\n",
        "# Initialize manager\n",
        "vector_manager = VectorStoreManager(collection, embedder)\n",
        "\n",
        "# Add upload endpoint to existing FastAPI app\n",
        "@app.post(\"/upload\")\n",
        "async def upload_document(\n",
        "    file: UploadFile = File(...),\n",
        "    category: str = Form(\"general\")\n",
        "):\n",
        "    \"\"\"Upload and process PDF, DOCX, or TXT document\"\"\"\n",
        "    try:\n",
        "        # Validate file type\n",
        "        allowed_extensions = ['.pdf', '.docx', '.txt']\n",
        "        file_ext = os.path.splitext(file.filename)[1].lower()\n",
        "\n",
        "        if file_ext not in allowed_extensions:\n",
        "            return JSONResponse(\n",
        "                status_code=400,\n",
        "                content={\n",
        "                    \"success\": False,\n",
        "                    \"error\": f\"Unsupported file type. Allowed: {', '.join(allowed_extensions)}\"\n",
        "                }\n",
        "            )\n",
        "\n",
        "        # Save uploaded file\n",
        "        file_path = f\"/tmp/{file.filename}\"\n",
        "        with open(file_path, \"wb\") as f:\n",
        "            content = await file.read()\n",
        "            f.write(content)\n",
        "\n",
        "        # Process document\n",
        "        docs = DocumentProcessor.process_document(file_path, category)\n",
        "\n",
        "        if not docs:\n",
        "            return JSONResponse(\n",
        "                status_code=400,\n",
        "                content={\"success\": False, \"error\": \"Failed to extract text from document\"}\n",
        "            )\n",
        "\n",
        "        # Add to vector store\n",
        "        chunks_added = vector_manager.add_documents(docs, category)\n",
        "\n",
        "        # Clean up temp file\n",
        "        os.remove(file_path)\n",
        "\n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"filename\": file.filename,\n",
        "            \"chunks_created\": chunks_added,\n",
        "            \"category\": category,\n",
        "            \"message\": f\"Successfully processed {chunks_added} chunks\"\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        return JSONResponse(\n",
        "            status_code=500,\n",
        "            content={\"success\": False, \"error\": str(e)}\n",
        "        )\n",
        "\n",
        "@app.get(\"/stats\")\n",
        "def get_stats():\n",
        "    \"\"\"Get vector store statistics\"\"\"\n",
        "    return vector_manager.get_stats()\n",
        "\n",
        "@app.delete(\"/documents/{category}\")\n",
        "def delete_category(category: str):\n",
        "    \"\"\"Delete all documents in a category\"\"\"\n",
        "    try:\n",
        "        vector_manager.delete_by_category(category)\n",
        "        return {\"success\": True, \"deleted_category\": category}\n",
        "    except Exception as e:\n",
        "        return JSONResponse(status_code=500, content={\"success\": False, \"error\": str(e)})\n",
        "\n",
        "print(\"‚úÖ Document Upload API ready!\")\n",
        "print(\"üì§ Endpoints:\")\n",
        "print(\"   POST /upload - Upload PDF/DOCX/TXT\")\n",
        "print(\"   GET  /stats  - Get statistics\")\n",
        "print(\"   DELETE /documents/{category} - Delete category\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcbDKR5Ju5xG",
        "outputId": "e98fccf3-2007-4a4f-8434-4f4626c9dcaa"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Application startup complete.\n",
            "ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 8000): address already in use\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n"
          ]
        }
      ]
    }
  ]
}